##集群模块_数据迁移子系统

##数据迁移子系统概述

    我们现在定义本系统的数据迁移只会发生在两种情况下：
    @增删集群中的节点
    @指定的数据迁移
    
    除了以上两种情况我们不做任何数据迁移定义。还有一种情况，整个集群需要迁移或切换，这里暂时不考虑。
    
##增删集群中的节点

![ss](../image/数据迁移状态转移图示.png)

数据迁移方式：
    
    同构与异构方式的结合。
    同构方式：
        所有的备份相同，当有一个节点变化的时候，我们需要将所有数据迁移到一个指定的位置。
    异构方式：
        当一个节点变化时，需要多个节点联合恢复一个节点或者迁移数据。
        
    结构分析：
        如果使用同构方式，数据的迁移影响范围较小，但是当数据量比较小的时候，这种方式是很合
        适的。但是如果数据量很大，那么这两个节点之间会持续很长时间传送数据，这就严重影响到了
        性能。接着我们讨论异构方式，异构方式是将数据备份分散在多个节点上，当一个节点作用
        的时候，其它多个节点同时帮助一个节点恢复或者负载数据，直接提高了速度，自动负载均衡
        但是这样操作的复杂性就会直接提高。毕竟异构要比同构更加的难以控制。
     我们的抉择：
        我们需要衡量出一个适合我们的数据迁移结构，其实在我们选择一致性哈希的时候已经定下来了，
        但是我们需要分析是否适合我们的设计目标，项目需求。在这次的项目中，我们是采用了异构与
        同构结合的方式合并的方式，在每个节点上采取主从的模型，进行同步的方式组和数据的一致性
        在整个集群上采取异步的方式，一个节点的变化引导整个集群联动，直接加速。再次考虑到我们
        的大数据量，我们需要采用这样的方式。
        
##指定数据迁移
        这个数据迁移是宏观的方式，即我们是采取一个P2P的方式直接点对点迁移所有的数据。
        根据之前设计，如果采取点到点的数据迁移我们会发现，暂时只能采取增加虚拟节点的
        方式，这样策略使用过多，直接会导致我们这个集群的负载均衡被打破，所以我们是否
        需要使用这种方式来进行P2P的数据迁移是需要思考的，我们甚至需考虑是否需要提供P2P
        的数据迁移这种机制，事实上是必须的，但是纵观我们已经存在的节点增删机制，我们
        需要好好考虑一种机制，能够在不影响全局大的机制上，演绎出一个迁移的机制。
        
        分析：
             我们需要一个将数据迁移到指定数据库的机制。当前已有机制：数据节点增删的
             自动负载均衡机制。我们基于以上前提条件来考虑以下的所有情况：
               @迁移全部数据到一个新的服务器。
               @数据迁移到我们的一个已有的服务器。
               
             迁移全部数据到一个新的服务器，首先进行同步的数据迁移，然后修改全局路由
             表将旧的结点的IP替换成新结点的IP。
             
             数据迁移到一个已经存在的服务器，此处我们认为这是无意义的，我们直接等价
             与删除这个节点，直接使用我们已经存在的删除节点的策略就可以解决。
             
        
        